# wait_for_model.py
import time
from backend.llm.client import _get_client, _ensure_pipeline_ready

def wait_for_model_complete():
    """ç­‰å¾…æ¨¡å‹å®Œå…¨å°±ç»ª"""
    print("â³ ç­‰å¾…æ¨¡å‹å®Œå…¨å°±ç»ª...")
    
    client = _get_client()
    max_wait = 300  # 5åˆ†é’Ÿè¶…æ—¶
    start_time = time.time()
    
    while time.time() - start_time < max_wait:
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        if (_ensure_pipeline_ready(client) and 
            client._pipeline is not None and 
            client._pipeline.model is not None):
            
            # æµ‹è¯•çœŸå®æ¨ç†
            try:
                from backend.taxi.state_utils import describe_state_for_llm, get_prompt
                
                EXAMPLE_DECODED_STATE = {
                    "taxi_row": 2, "taxi_col": 1,
                    "passenger_location": 4, "destination_index": 3,
                }
                
                state_desc = describe_state_for_llm(EXAMPLE_DECODED_STATE)
                prompt = get_prompt(state_desc)
                
                result = client.generate(prompt)
                if result.get('action') is not None:
                    print("âœ… æ¨¡å‹å®Œå…¨å°±ç»ªï¼")
                    print(f"ç­‰å¾…æ—¶é—´: {time.time() - start_time:.0f}ç§’")
                    return True
                    
            except Exception as e:
                print(f"æ¨¡å‹æµ‹è¯•ä¸­: {e}")
        
        print(f"æ¨¡å‹åˆå§‹åŒ–ä¸­... å·²ç­‰å¾… {int(time.time() - start_time)}ç§’")
        time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
    
    print("âŒ æ¨¡å‹åˆå§‹åŒ–è¶…æ—¶")
    return False

# ç­‰å¾…æ¨¡å‹
if wait_for_model_complete():
    print("ğŸ‰ ç°åœ¨å¯ä»¥è¿è¡Œspeed_testäº†ï¼")
else:
    print("ğŸ’¥ éœ€è¦é‡å¯Colabæˆ–æ£€æŸ¥ç½‘ç»œ")


# check_download.py
import os
import requests

def check_download_progress():
    """æ£€æŸ¥æ¨¡å‹ä¸‹è½½è¿›åº¦"""
    print("ğŸ“¥ æ£€æŸ¥æ¨¡å‹ä¸‹è½½è¿›åº¦")
    print("=" * 30)
    
    # Hugging Faceç¼“å­˜è·¯å¾„
    cache_path = "/root/.cache/huggingface/hub"
    model_path = os.path.join(cache_path, "models--Qwen")
    
    if os.path.exists(model_path):
        print("âœ… æ¨¡å‹ç¼“å­˜ç›®å½•å­˜åœ¨")
        
        # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
        safetensors_files = []
        for root, dirs, files in os.walk(model_path):
            for file in files:
                if file.endswith('.safetensors'):
                    safetensors_files.append(os.path.join(root, file))
                elif file.endswith('.bin'):
                    print(f"æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {os.path.join(root, file)}")
        
        if safetensors_files:
            print(f"âœ… æ‰¾åˆ° {len(safetensors_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")
            for file in safetensors_files[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                size = os.path.getsize(file) / (1024*1024*1024)  # GB
                print(f"  {os.path.basename(file)}: {size:.2f} GB")
        else:
            print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶")
    else:
        print("âŒ æ¨¡å‹ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")

check_download_progress()

# quick_status.py
from backend.llm.client import _get_client

def quick_status():
    """å¿«é€ŸçŠ¶æ€æ£€æŸ¥"""
    print("ğŸ” å¿«é€ŸçŠ¶æ€æ£€æŸ¥")
    print("=" * 30)
    
    client = _get_client()
    
    print(f"Pipelineå­˜åœ¨: {client._pipeline is not None}")
    print(f"æ¨¡å‹å­˜åœ¨: {getattr(client._pipeline, 'model', None) is not None if client._pipeline else False}")
    
    if client._pipeline and client._pipeline.model:
        try:
            # å°è¯•è·å–å‚æ•°æ•°é‡ï¼ˆå¦‚æœæ¨¡å‹å·²åŠ è½½ï¼‰
            param_count = sum(p.numel() for p in client._pipeline.model.parameters())
            print(f"æ¨¡å‹å‚æ•°: {param_count:,}")
            print("âœ… æ¨¡å‹å·²åŠ è½½å®Œæˆ")
        except:
            print("âš ï¸ æ¨¡å‹å¯¹è±¡å­˜åœ¨ä½†å¯èƒ½æœªå®Œå…¨åˆå§‹åŒ–")

quick_status()