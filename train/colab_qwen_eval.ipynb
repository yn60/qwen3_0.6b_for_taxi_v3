{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73de0f8b",
   "metadata": {},
   "source": [
    "# Qwen3-0.6B batch evaluation on Taxi-v3\n",
    "\n",
    "This notebook runs multiple Taxi-v3 episodes where the action is chosen by the local Qwen3-0.6B model via your backend (`backend/llm/client.py`, `backend/taxi/*`). It works on Colab and locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd347a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab, install dependencies\n",
    "import sys, os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    # Minimal set; torch and transformers are often present, but upgrade to ensure correct versions\n",
    "    !pip -q install -U gymnasium==0.26.2 transformers accelerate torch safetensors sentencepiece huggingface_hub\n",
    "\n",
    "# Ensure the backend package path is available (repo structure: backend/taxi, backend/llm)\n",
    "repo_root = os.getcwd()\n",
    "backend_path = os.path.join(repo_root, 'backend')\n",
    "if backend_path not in sys.path:\n",
    "    sys.path.append(backend_path)\n",
    "\n",
    "# On Colab, prefer GPU if available for Qwen\n",
    "if IN_COLAB and 'QWEN_DEVICE' not in os.environ:\n",
    "    import torch\n",
    "    os.environ['QWEN_DEVICE'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Optionally control generation behavior\n",
    "os.environ.setdefault('QWEN_TEMPERATURE', '0.2')\n",
    "# You can set QWEN_MAX_NEW_TOKENS to limit output; default uses a safe value from the client\n",
    "# os.environ['QWEN_MAX_NEW_TOKENS'] = '256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223152a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from the backend\n",
    "from taxi.environment import TaxiEnvironment\n",
    "from taxi.state_utils import decode_state, describe_state_for_llm, get_prompt\n",
    "from llm.client import get_qwen_action, _ensure_pipeline_ready, _get_client\n",
    "# Use reusable action coercion helper (no Flask dependency)\n",
    "from taxi.action_utils import coerce_action\n",
    "\n",
    "import json, time\n",
    "from typing import Dict, Any, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm up the Qwen pipeline (downloads model if needed)\n",
    "client = _get_client()\n",
    "ready = _ensure_pipeline_ready(client)\n",
    "if not ready:\n",
    "    print('Model is loading in the background... waiting briefly (up to ~60s)')\n",
    "    # Poll for a short while\n",
    "    for _ in range(60):\n",
    "        time.sleep(1)\n",
    "        if _ensure_pipeline_ready(client):\n",
    "            break\n",
    "print('Pipeline ready:', _ensure_pipeline_ready(client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f31492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_policy_action(state: int) -> Tuple[int, Dict[str, Any]]:\n",
    "    \"\"\"Given an environment integer state, query Qwen for an action.\n",
    "    Returns (action_code, full_llm_payload). If the action can't be coerced, returns (None, payload).\n",
    "    \"\"\"\n",
    "    state_desc = describe_state_for_llm(decode_state(state))\n",
    "    prompt = get_prompt(state_desc)\n",
    "    result = get_qwen_action(prompt)\n",
    "    action = coerce_action(result.get('action'))\n",
    "    return action, result\n",
    "\n",
    "def run_episode(max_steps: int = 200, verbose: bool = False) -> Dict[str, Any]:\n",
    "    env = TaxiEnvironment()\n",
    "    state = env.observation\n",
    "    total_reward = 0.0\n",
    "    steps = []\n",
    "    success = False\n",
    "    for t in range(max_steps):\n",
    "        action, payload = qwen_policy_action(state)\n",
    "        if action is None:\n",
    "            # If the model is not ready or returns invalid output, end early\n",
    "            return {\n",
    "                'success': False,\n",
    "                'ended_early': True,\n",
    "                'reason': 'Invalid/empty action from LLM',\n",
    "                'total_reward': total_reward,\n",
    "                'steps': steps,\n",
    "                'llm_last': payload,\n",
    "            }\n",
    "        next_state, reward, done = env.step(action)\n",
    "        steps.append({\n",
    "            'state': int(state),\n",
    "            'action': int(action),\n",
    "            'reward': float(reward),\n",
    "        })\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if verbose:\n",
    "            print(f'Step {t}: action={action}, reward={reward}, total={total_reward}')\n",
    "        if done:\n",
    "            success = (reward == 20)  # Taxi-v3 gives +20 on successful drop-off\n",
    "            break\n",
    "    env.close()\n",
    "    return {\n",
    "        'success': bool(success),\n",
    "        'ended_early': False,\n",
    "        'total_reward': float(total_reward),\n",
    "        'steps': steps,\n",
    "    }\n",
    "\n",
    "def run_batch(n_episodes: int = 20, max_steps: int = 200, verbose_every: int = 0) -> Dict[str, Any]:\n",
    "    results = []\n",
    "    successes = 0\n",
    "    total_rewards = 0.0\n",
    "    for i in range(n_episodes):\n",
    "        ep = run_episode(max_steps=max_steps, verbose=False)\n",
    "        results.append(ep)\n",
    "        successes += 1 if ep.get('success') else 0\n",
    "        total_rewards += ep.get('total_reward', 0.0)\n",
    "        if verbose_every and (i + 1) % verbose_every == 0:\n",
    "            print(f\"Episode {i+1}/{n_episodes}: success={ep.get('success')}, total_reward={ep.get('total_reward')}\")\n",
    "    avg_reward = total_rewards / max(1, n_episodes)\n",
    "    return {\n",
    "        'episodes': results,\n",
    "        'success_rate': successes / max(1, n_episodes),\n",
    "        'average_reward': avg_reward,\n",
    "        'count': n_episodes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the batch\n",
    "metrics = run_batch(n_episodes=10, max_steps=200, verbose_every=1)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df86088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save detailed results to JSON\n",
    "out = 'qwen_taxi_results.json'\n",
    "with open(out, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print('Saved to', out)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
